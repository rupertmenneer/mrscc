{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANLE - Assignment 1 (unigram/bigram)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QFwJxWp1QqQ"
      },
      "source": [
        "# Advanced Natural Language Engineering - Assignment 1\n",
        "\n",
        "This assignment asks us to compete in The Microsoft Research Sentence Completion Challenge - MRSCC (Zweig and Burges, 2011), it requires a system to be able to predict which is the most likely word (from a set of 5 possibilities) to complete a sentence. \n",
        "\n",
        "There are 4 different methods that will be compared in this challenge:\n",
        "\n",
        "1.   Unigram model\n",
        "2.   Bigram model\n",
        "3. Maxium Entropy model (Logisitic Regression)\n",
        "4. RoBERTa model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTzc8oiJ2cZ4"
      },
      "source": [
        "### Loading challenge data\n",
        "\n",
        "For this challenge we are provided with:\n",
        "\n",
        "1.   A training corpus of 19th century novels data (522 files)\n",
        "2.   1040 sentences with one missing word and 5 options to choose from\n",
        "\n",
        "This dataset was constructed from Project Gutenberg data. Seed sentences were selected from five of Sir\n",
        "Arthur Conan Doyleâ€™s Sherlock Holmes novels, and then imposter words were suggested with the\n",
        "aid of a language model trained on over 500 19th century novels. The strategy for competing in this challenge will be to create training and validation data from the complete corpus. This will then help us make predictions in the unseen MRSCC challenge data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw9T2bSWEocL"
      },
      "source": [
        "%%capture\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import os\n",
        "import random\n",
        "from nltk import word_tokenize as tokenize\n",
        "import operator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR8ja4Ff2k-N",
        "outputId": "a9559367-c1ef-40bf-acc3-c88cfd079add"
      },
      "source": [
        "mrscc_dir = '/content/drive/MyDrive/university/2021/ANLE/lab2resources/sentence-completion'\n",
        "\n",
        "def get_train_val(training_dir=mrscc_dir,split=0.2):\n",
        "    filenames=os.listdir(training_dir)\n",
        "    n=len(filenames)\n",
        "    print(\"There are {} files in the training directory: {}\".format(n,training_dir))\n",
        "    random.seed(7) #if you want the same random split every time\n",
        "    random.shuffle(filenames)\n",
        "    index=int(n*split)\n",
        "    return(filenames[:index],filenames[index:])\n",
        "\n",
        "trainingdir=os.path.join(mrscc_dir,\"Holmes_Training_Data\")\n",
        "training,testing=get_train_val(trainingdir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 522 files in the training directory: /content/drive/MyDrive/university/2021/ANLE/lab2resources/sentence-completion/Holmes_Training_Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztCn8dTI2YYH"
      },
      "source": [
        "### 1. Unigram & Bigram model\n",
        "\n",
        "This language model has been taken and adapted from the ANLE lab resources."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBod-uWr1Ni3"
      },
      "source": [
        "class language_model():\n",
        "    \n",
        "    def __init__(self,trainingdir=mrscc_dir,files=[]):\n",
        "        self.training_dir=trainingdir\n",
        "        self.files=files\n",
        "        self.train()\n",
        "        \n",
        "    def train(self):    \n",
        "        self.unigram={}\n",
        "        self.bigram={}\n",
        "         \n",
        "        self._processfiles()\n",
        "        self._make_unknowns()\n",
        "        self._discount()\n",
        "        self._convert_to_probs()\n",
        "        \n",
        "    \n",
        "    def _processline(self,line, i):\n",
        "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
        "        previous=\"__END\"\n",
        "        for token in tokens:\n",
        "            self.unigram[token]=self.unigram.get(token,0)+1\n",
        "            current=self.bigram.get(previous,{})\n",
        "            current[token]=current.get(token,0)+1\n",
        "            self.bigram[previous]=current\n",
        "            previous=token\n",
        "            \n",
        "    \n",
        "    def _processfiles(self):\n",
        "      for i, afile in enumerate(self.files):\n",
        "          # print(\"Processing {}\".format(afile))\n",
        "          try:\n",
        "              with open(os.path.join(self.training_dir,afile)) as instream:\n",
        "                  for line in instream:\n",
        "                      line=line.rstrip()\n",
        "                      if len(line)>0:\n",
        "                          self._processline(line, i)\n",
        "          except UnicodeDecodeError:\n",
        "              print(\"UnicodeDecodeError processing {}: ignoring rest of file\".format(afile))\n",
        "      print('file processing complete')\n",
        "      \n",
        "            \n",
        "    def _convert_to_probs(self):\n",
        "        self.unigram={k:v/sum(self.unigram.values()) for (k,v) in self.unigram.items()}\n",
        "        self.bigram={key:{k:v/sum(adict.values()) for (k,v) in adict.items()} for (key,adict) in self.bigram.items()}\n",
        "        self.kn={k:v/sum(self.kn.values()) for (k,v) in self.kn.items()}\n",
        "        \n",
        "    def get_prob(self,token,context=\"\",methodparams={}):\n",
        "        if methodparams.get(\"method\",\"unigram\")==\"unigram\":\n",
        "            return self.unigram.get(token,self.unigram.get(\"__UNK\",0))\n",
        "        else:\n",
        "            if methodparams.get(\"smoothing\",\"kneser-ney\")==\"kneser-ney\":\n",
        "                unidist=self.kn\n",
        "            else:\n",
        "                unidist=self.unigram\n",
        "            bigram=self.bigram.get(context[-1],self.bigram.get(\"__UNK\",{}))\n",
        "            big_p=bigram.get(token,bigram.get(\"__UNK\",0))\n",
        "            lmbda=bigram[\"__DISCOUNT\"]\n",
        "            uni_p=unidist.get(token,unidist.get(\"__UNK\",0))\n",
        "            #print(big_p,lmbda,uni_p)\n",
        "            p=big_p+lmbda*uni_p            \n",
        "            return p\n",
        "    \n",
        "    \n",
        "    def nextlikely(self,k=1,current=\"\",method=\"unigram\"):\n",
        "        #use probabilities according to method to generate a likely next sequence\n",
        "        #choose random token from k best\n",
        "        blacklist=[\"__START\",\"__UNK\",\"__DISCOUNT\"]\n",
        "       \n",
        "        if method==\"unigram\":\n",
        "            dist=self.unigram\n",
        "        else:\n",
        "            dist=self.bigram.get(current,self.bigram.get(\"__UNK\",{}))    \n",
        "        #sort the tokens by unigram probability\n",
        "        mostlikely=sorted(list(dist.items()),key=operator.itemgetter(1),reverse=True)\n",
        "        #filter out any undesirable tokens\n",
        "        filtered=[w for (w,p) in mostlikely if w not in blacklist]\n",
        "        #choose one randomly from the top k\n",
        "        res=random.choice(filtered[:k])\n",
        "        return res\n",
        "    \n",
        "    def generate(self,k=1,end=\"__END\",limit=20,method=\"bigram\",methodparams={}):\n",
        "        if method==\"\":\n",
        "            method=methodparams.get(\"method\",\"bigram\")\n",
        "        current=\"__START\"\n",
        "        tokens=[]\n",
        "        while current!=end and len(tokens)<limit:\n",
        "            current=self.nextlikely(k=k,current=current,method=method)\n",
        "            tokens.append(current)\n",
        "        return \" \".join(tokens[:-1])\n",
        "    \n",
        "    \n",
        "    def compute_prob_line(self,line,methodparams={}):\n",
        "        #this will add _start to the beginning of a line of text\n",
        "        #compute the probability of the line according to the desired model\n",
        "        #and returns probability together with number of tokens       \n",
        "        tokens=[\"__START\"]+tokenize(line)+[\"__END\"]\n",
        "        acc=0\n",
        "        for i,token in enumerate(tokens[1:]):\n",
        "            acc+=math.log(self.get_prob(token,tokens[:i+1],methodparams))\n",
        "        return acc,len(tokens[1:])\n",
        "    \n",
        "    def compute_probability(self,filenames=[],methodparams={}):\n",
        "        #computes the probability (and length) of a corpus contained in filenames\n",
        "        if filenames==[]:\n",
        "            filenames=self.files      \n",
        "        total_p=0\n",
        "        total_N=0\n",
        "        for i,afile in enumerate(filenames):\n",
        "            print(\"Processing file {}:{}\".format(i,afile))\n",
        "            try:\n",
        "                with open(os.path.join(self.training_dir,afile)) as instream:\n",
        "                    for line in instream:\n",
        "                        line=line.rstrip()\n",
        "                        if len(line)>0:\n",
        "                            p,N=self.compute_prob_line(line,methodparams=methodparams)\n",
        "                            total_p+=p\n",
        "                            total_N+=N\n",
        "            except UnicodeDecodeError:\n",
        "                print(\"UnicodeDecodeError processing file {}: ignoring rest of file\".format(afile))\n",
        "        return total_p,total_N\n",
        "    \n",
        "    def compute_perplexity(self,filenames=[],methodparams={\"method\":\"bigram\",\"smoothing\":\"kneser-ney\"}):\n",
        "        #compute the probability and length of the corpus\n",
        "        #calculate perplexity\n",
        "        #lower perplexity means that the model better explains the data\n",
        "        p,N=self.compute_probability(filenames=filenames,methodparams=methodparams)\n",
        "        #print(p,N)\n",
        "        pp=math.exp(-p/N)\n",
        "        return pp\n",
        "    \n",
        "    def compute_line_perplexity(self, line):\n",
        "        line_prob, line_len = self.compute_prob_line(line)\n",
        "        return math.exp(-line_prob/line_len)\n",
        "    \n",
        "    def _make_unknowns(self,known=2):\n",
        "        unknown=0\n",
        "        for (k,v) in list(self.unigram.items()):\n",
        "            if v<known:\n",
        "                del self.unigram[k]\n",
        "                self.unigram[\"__UNK\"]=self.unigram.get(\"__UNK\",0)+v\n",
        "        for (k,adict) in list(self.bigram.items()):\n",
        "            for (kk,v) in list(adict.items()):\n",
        "                isknown=self.unigram.get(kk,0)\n",
        "                if isknown==0:\n",
        "                    adict[\"__UNK\"]=adict.get(\"__UNK\",0)+v\n",
        "                    del adict[kk]\n",
        "            isknown=self.unigram.get(k,0)\n",
        "            if isknown==0:\n",
        "                del self.bigram[k]\n",
        "                current=self.bigram.get(\"__UNK\",{})\n",
        "                current.update(adict)\n",
        "                self.bigram[\"__UNK\"]=current\n",
        "                \n",
        "            else:\n",
        "                self.bigram[k]=adict\n",
        "                \n",
        "    def _discount(self,discount=0.75):\n",
        "        #discount each bigram count by a small fixed amount\n",
        "        self.bigram={k:{kk:value-discount for (kk,value) in adict.items()}for (k,adict) in self.bigram.items()}\n",
        "        \n",
        "        #for each word, store the total amount of the discount so that the total is the same \n",
        "        #i.e., so we are reserving this as probability mass\n",
        "        for k in self.bigram.keys():\n",
        "            lamb=len(self.bigram[k])\n",
        "            self.bigram[k][\"__DISCOUNT\"]=lamb*discount\n",
        "            \n",
        "        #work out kneser-ney unigram probabilities\n",
        "        #count the number of contexts each word has been seen in\n",
        "        self.kn={}\n",
        "        for (k,adict) in self.bigram.items():\n",
        "            for kk in adict.keys():\n",
        "                self.kn[kk]=self.kn.get(kk,0)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTc1cLOBB0yp",
        "outputId": "7c3bf566-500c-4d34-dd1c-be0cf06f2694"
      },
      "source": [
        "MAX_FILES=20\n",
        "mylm=language_model(trainingdir=trainingdir,files=training[:MAX_FILES])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UnicodeDecodeError processing TNGLW10.TXT: ignoring rest of file\n",
            "file processing complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-LI3eYcFWNy"
      },
      "source": [
        "vocab=sorted(mylm.unigram.items(),key=lambda x:x[1],reverse=True)\n",
        "# vocab[:20]  \n",
        "# mylm.bigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "sSbXlJEfMUFC",
        "outputId": "8559a58c-da10-48e0-8803-394aab83a3a2"
      },
      "source": [
        "import pandas as pd, csv\n",
        "questions=pd.read_csv(os.path.join(mrscc_dir,\"testing_data.csv\"))\n",
        "answers=pd.read_csv(os.path.join(mrscc_dir,\"test_answer.csv\"))\n",
        "\n",
        "questions.rename(columns={'a)':'a','b)':'b','c)':'c','d)':'d','e)':'e'}, inplace=True)\n",
        "questions.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "      <th>e</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>I have it from the same source that you are bo...</td>\n",
              "      <td>crying</td>\n",
              "      <td>instantaneously</td>\n",
              "      <td>residing</td>\n",
              "      <td>matched</td>\n",
              "      <td>walking</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>It was furnished partly as a sitting and partl...</td>\n",
              "      <td>daintily</td>\n",
              "      <td>privately</td>\n",
              "      <td>inadvertently</td>\n",
              "      <td>miserably</td>\n",
              "      <td>comfortably</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>As I descended , my old ally , the _____ , cam...</td>\n",
              "      <td>gods</td>\n",
              "      <td>moon</td>\n",
              "      <td>panther</td>\n",
              "      <td>guard</td>\n",
              "      <td>country-dance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>We got off , _____ our fare , and the trap rat...</td>\n",
              "      <td>rubbing</td>\n",
              "      <td>doubling</td>\n",
              "      <td>paid</td>\n",
              "      <td>naming</td>\n",
              "      <td>carrying</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>He held in his hand a _____ of blue paper , sc...</td>\n",
              "      <td>supply</td>\n",
              "      <td>parcel</td>\n",
              "      <td>sign</td>\n",
              "      <td>sheet</td>\n",
              "      <td>chorus</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...              e\n",
              "0   1  ...        walking\n",
              "1   2  ...    comfortably\n",
              "2   3  ...  country-dance\n",
              "3   4  ...       carrying\n",
              "4   5  ...         chorus\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY2D-A6x9e50"
      },
      "source": [
        "class question:\n",
        "    \n",
        "    def __init__(self,aline, lm):\n",
        "        self.sentence=aline[1]\n",
        "        self.choices = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
        "        self.word_choices = {index:word for index,word in zip(self.choices,aline[2:])}\n",
        "        self.lm = lm\n",
        "\n",
        "    def add_answer(self,fields):\n",
        "        self.answer=fields[1]\n",
        "   \n",
        "    def chooseA(self):\n",
        "        return(\"a\")\n",
        "    \n",
        "    def chooseRandom(self):\n",
        "        return random.choice(self.choices)\n",
        "    \n",
        "    def chooseUnigram(self):     \n",
        "        # matches choices with words    \n",
        "        words = self.word_choices.values()\n",
        "        # make dictionary keys:values {take first character e.g. a: word probability from unigram}          \n",
        "        probabilities = {index[0]:self.lm.unigram.get(word,0) for index,word in zip(self.choices, words)}\n",
        "        # sort probs         \n",
        "        sorted_probabilities = dict(sorted(probabilities.items(), key=lambda item: item[1], reverse=True))\n",
        "        # take first key (highest probability word but return key)     \n",
        "        return list(sorted_probabilities.keys())[0]\n",
        "\n",
        "    def chooseBigramLeft(self, left_word):\n",
        "      # get bigram dict for this word\n",
        "        bigram_for_left_word = self.lm.bigram.get(left_word, 0)\n",
        "      # get probabilites of each bigram P(w-1|w) - if exists\n",
        "        bigram_probs_for_left = {key:bigram_for_left_word.get(w, 0) for key,w in self.word_choices.items() if bigram_for_left_word != 0}\n",
        "        return dict(sorted(bigram_probs_for_left.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    def chooseBigramRight(self, right_word):\n",
        "        # get bigram dict for each word choice\n",
        "        bigram_for_word_choice = {key:self.lm.bigram.get(w, 0) for key,w in self.word_choices.items()}\n",
        "        # get probs for each choice and word to right\n",
        "        bigram_probs_for_right = {key:b.get(right_word, 0) for key,b in bigram_for_word_choice.items() if b != 0}\n",
        "        return dict(sorted(bigram_probs_for_right.items(), key=lambda item: item[1], reverse=True))\n",
        "\n",
        "    def chooseBigram(self, left, right):\n",
        "        context = self.get_window_context(self.sentence, 1, 1)\n",
        "\n",
        "        # ---------- left bigram\n",
        "        if left:\n",
        "          l = self.chooseBigramLeft(context[0])\n",
        "          if not right:\n",
        "            # if no bigram for left context or max prob is still 0\n",
        "            if not l or list(l.values())[0] == 0:\n",
        "              return self.chooseUnigram()\n",
        "            return list(l.keys())[0]\n",
        "\n",
        "        # ---------- right bigram\n",
        "        if right:\n",
        "          r = self.chooseBigramRight(context[-1])\n",
        "          if not left:\n",
        "            # if no bigram for left context or all probs zero resort to unigram\n",
        "            if not r or list(r.values())[0] == 0:\n",
        "              return self.chooseUnigram()\n",
        "            return list(r.keys())[0]\n",
        "\n",
        "        # ---------- both contexts\n",
        "        if right and left:\n",
        "          bigram_both = {key:bigram*r[key] for key,bigram in l.items() if key in r.keys()}\n",
        "          bigram_both = dict(sorted(bigram_both.items(), key=lambda item: item[1], reverse=True))\n",
        "          if not r or not l:\n",
        "            return self.chooseUnigram()\n",
        "        return list(bigram_both.keys())[0]\n",
        "        \n",
        "    def predict_and_score(self,method=\"chooseA\", left=None, right=None):\n",
        "        #compare prediction according to method with the correct answer\n",
        "        #return 1 or 0 accordingly\n",
        "        prediction=self.predict(method, left, right)\n",
        "        if prediction ==self.answer:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "        \n",
        "    def get_window_context(self,sent_tokens,window_left, window_right,target=\"_____\"):\n",
        "        tokens=tokenize(sent_tokens)\n",
        "        # print(tokens)\n",
        "        found=False\n",
        "        for i,token in enumerate(tokens):\n",
        "            if token==target:\n",
        "                found=True\n",
        "                break \n",
        "        if found:\n",
        "            return tokens[i-window_left:i+1+window_right]\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def predict(self,method=\"chooseA\", left=None, right=None):\n",
        "        #eventually there will be lots of methods to choose from\n",
        "        if method==\"chooseA\":\n",
        "            return self.chooseA()\n",
        "        if method==\"random\":\n",
        "            return self.chooseRandom()\n",
        "        if method==\"unigram\":\n",
        "            return self.chooseUnigram()\n",
        "        if method==\"bigram\":\n",
        "            return self.chooseBigram(left, right)\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTf-E3jokV3a"
      },
      "source": [
        "class scc_reader:\n",
        "    \n",
        "    def __init__(self, lm, qs=questions, ans=answers):\n",
        "        self.qs=qs\n",
        "        self.ans=ans\n",
        "        self.lm = lm\n",
        "        self.read_files()\n",
        "   \n",
        "    def read_files(self):\n",
        "        #create a question instance for each line of the file (other than heading line)\n",
        "        self.questions=[question(questions.iloc[i], self.lm) for i in range(len(questions))]\n",
        "        #add answers to questions so predictions can be checked    \n",
        "        for i,q in enumerate(self.questions):\n",
        "            q.add_answer(answers.iloc[i])\n",
        "        \n",
        "    def get_field(self,field):\n",
        "        return [q.get_field(field) for q in self.questions] \n",
        "    \n",
        "    def predict(self,method=\"chooseA\"):\n",
        "        return [q.predict(method=method) for q in self.questions]\n",
        "    \n",
        "    def predict_and_score(self,method=\"chooseA\", left=None, right=None):\n",
        "        scores=[q.predict_and_score(method, left, right) for q in self.questions]\n",
        "        return sum(scores)/len(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrbL2fnCMnL_"
      },
      "source": [
        "SCC = scc_reader(lm=mylm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWmrVcXAu_oi",
        "outputId": "baf3bfdc-e6c0-48de-fb73-3b098ab61612"
      },
      "source": [
        "SCC.predict_and_score('random')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2173076923076923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgQpyOBAMzvz",
        "outputId": "43d617e2-15a0-484e-c356-7e44c35fa5d5"
      },
      "source": [
        "print('random {}'.format(SCC.predict_and_score('random')))\n",
        "print('unigram {}'.format(SCC.predict_and_score('unigram')))\n",
        "print('bigram left {}'.format(SCC.predict_and_score(\"bigram\", 1, 0)))\n",
        "print('bigram right {}'.format(SCC.predict_and_score(\"bigram\", 0, 1)))\n",
        "print('bigram both {}'.format(SCC.predict_and_score(\"bigram\", 1, 1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "random 0.20384615384615384\n",
            "unigram 0.24711538461538463\n",
            "bigram left 0.2605769230769231\n",
            "bigram right 0.2048076923076923\n",
            "bigram both 0.23653846153846153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxV4KY6_Mn9w"
      },
      "source": [
        "def training_data_by_accuracy(max_iters):\n",
        "  accuracy_per_model = {'unigram':[], 'bigram left':[], 'bigram right':[], 'bigram both':[]}\n",
        "  for max in max_iters:\n",
        "    lm=language_model(trainingdir=trainingdir,files=training[:max])\n",
        "    SCC = scc_reader(lm)\n",
        "    accuracy_per_model['unigram'].append(SCC.predict_and_score('unigram'))\n",
        "    accuracy_per_model['bigram left'].append(SCC.predict_and_score('bigram', 1, 0))\n",
        "    accuracy_per_model['bigram right'].append(SCC.predict_and_score('bigram', 0, 1))\n",
        "    accuracy_per_model['bigram both'].append(SCC.predict_and_score('bigram', 1, 1))\n",
        "  return accuracy_per_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0I43c6uN42u",
        "outputId": "0484917a-d43e-448b-c506-64eaf859f35d"
      },
      "source": [
        "test = training_data_by_accuracy([1,5,10,100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "file processing complete\n",
            "file processing complete\n",
            "UnicodeDecodeError processing TNGLW10.TXT: ignoring rest of file\n",
            "file processing complete\n",
            "UnicodeDecodeError processing TNGLW10.TXT: ignoring rest of file\n",
            "UnicodeDecodeError processing HHOHG10.TXT: ignoring rest of file\n",
            "file processing complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80j5H2y4WiVP",
        "outputId": "46160ff0-8037-4f63-b75b-236d930c69d7"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bigram both': [0.20673076923076922,\n",
              "  0.2326923076923077,\n",
              "  0.2375,\n",
              "  0.26826923076923076],\n",
              " 'bigram left': [0.2201923076923077,\n",
              "  0.26634615384615384,\n",
              "  0.2798076923076923,\n",
              "  0.27403846153846156],\n",
              " 'bigram right': [0.2144230769230769,\n",
              "  0.23173076923076924,\n",
              "  0.21153846153846154,\n",
              "  0.20865384615384616],\n",
              " 'unigram': [0.21923076923076923,\n",
              "  0.2605769230769231,\n",
              "  0.2548076923076923,\n",
              "  0.2567307692307692]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    }
  ]
}